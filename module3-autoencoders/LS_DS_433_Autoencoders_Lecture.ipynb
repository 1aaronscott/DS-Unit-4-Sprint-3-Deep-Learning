{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science\n",
    "\n",
    "*Unit 4, Sprint 3, Module 3*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "> An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner.[1][2] The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore signal “noise”. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "*At the end of the lecture you should be to*:\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "\n",
    "__Problem:__ Is it possible to automatically represent an image as a fixed-sized vector even if it isn’t labeled?\n",
    "\n",
    "__Solution:__ Use an autoencoder\n",
    "\n",
    "Why do we need to represent an image as a fixed-sized vector do you ask? \n",
    "\n",
    "* __Information Retrieval__\n",
    "    - [Reverse Image Search](https://en.wikipedia.org/wiki/Reverse_image_search)\n",
    "    - [Recommendation Systems - Content Based Filtering](https://en.wikipedia.org/wiki/Recommender_system#Content-based_filtering)\n",
    "* __Dimensionality Reduction__\n",
    "    - [Feature Extraction](https://www.kaggle.com/c/vsb-power-line-fault-detection/discussion/78285)\n",
    "    - [Manifold Learning](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)\n",
    "\n",
    "We've already seen *representation learning* when we talked about word embedding modelings during our NLP week. Today we're going to achieve a similiar goal on images using *autoencoders*. An autoencoder is a neural network that is trained to attempt to copy its input to its output. Usually they are restricted in ways that allow them to copy only approximately. The model often learns useful properties of the data, because it is forced to prioritize which aspecs of the input should be copied. The properties of autoencoders have made them an important part of modern generative modeling approaches. Consider autoencoders a special case of feed-forward networks (the kind we've been studying); backpropagation and gradient descent still work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Architecture (Learn)\n",
    "<a id=\"p1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The *encoder* compresses the input data and the *decoder* does the reverse to produce the uncompressed version of the data to create a reconstruction of the input as accurately as possible:\n",
    "\n",
    "<img src='https://miro.medium.com/max/1400/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png' width=800/>\n",
    "\n",
    "The learning process gis described simply as minimizing a loss function: \n",
    "$ L(x, g(f(x))) $\n",
    "\n",
    "- $L$ is a loss function penalizing $g(f(x))$ for being dissimiliar from $x$ (such as mean squared error)\n",
    "- $f$ is the encoder function\n",
    "- $g$ is the decoder function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along\n",
    "### Extremely Simple Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/lambda-ds6/mnist_ae/runs/ed5pbng6\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.13 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wandb/keras/__init__.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wandb.init(project=\"mnist_ae\", entity=\"lambda-ds6\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train, # x_train 2nd time b/c we want to predict input\n",
    "                epochs=1000,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose = False,\n",
    "                callbacks=[WandbCallback()]) #to get w&b to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Expected to talk about the components of autoencoder and their purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Autoencoder (Learn)\n",
    "<a id=\"p2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "As long as our architecture maintains an hourglass shape, we can continue to add layers and create a deeper network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(784,))\n",
    "\n",
    "encoded = Dense(128, activation='relu')(input_img)\n",
    "encoded = Dense(64, activation='relu')(encoded)\n",
    "encoded = Dense(32, activation='relu')(encoded) #This is the dry strawberry\n",
    "\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "decoded = Dense(128, activation='relu')(decoded)\n",
    "decoded = Dense(784, activation='sigmoid')(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/lambda-ds6/mnist_ae/runs/wx1rpys6\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.13 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.488558). Check your callbacks.\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.744294). Check your callbacks.\n",
      "CPU times: user 12min 53s, sys: 1min 13s, total: 14min 6s\n",
      "Wall time: 8min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa1085424a8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# compile & fit model\n",
    "\n",
    "autoencoder = Model(inputs=input_img, outputs=decoded)\n",
    "autoencoder.compile(optimizer='sgd',\n",
    "                   loss='binary_crossentropy'\n",
    "#                    metrics=['accuracy'] # no metric since just trying to match the input so only 'loss' is needed\n",
    "                   )\n",
    "\n",
    "\n",
    "wandb.init(project=\"mnist_ae\", entity=\"lambda-ds6\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train, # x_train 2nd time b/c we want to predict input\n",
    "                epochs=500,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose = False,\n",
    "                callbacks=[WandbCallback()]) #to get w&b to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 10  # how many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### Convolutional autoencoder\n",
    "\n",
    "> Since our inputs are images, it makes sense to use convolutional neural networks (convnets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolutional autoencoders --they simply perform much better.\n",
    "\n",
    "> Let's implement one. The encoder will consist in a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist in a stack of Conv2D and UpSampling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Create Model \n",
    "input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            Notebook configured with <a href=\"https://wandb.com\" target=\"_blank\">W&B</a>. You can <a href=\"https://app.wandb.ai/lambda-ds6/mnist_ae/runs/zq1826d2\" target=\"_blank\">open</a> the run page, or call <code>%%wandb</code>\n",
       "            in a cell containing your training loop to display live results.  Learn more in our <a href=\"https://docs.wandb.com/docs/integrations/jupyter.html\" target=\"_blank\">docs</a>.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.13 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (1.779744). Check your callbacks.\n",
      "WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.889892). Check your callbacks.\n",
      "CPU times: user 4min 53s, sys: 1min 11s, total: 6min 4s\n",
      "Wall time: 5min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa19aee4b38>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "wandb.init(project=\"mnist_ae\", entity=\"lambda-ds6\")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                verbose=False,\n",
    "                callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 20, not 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d90dc3a69dbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# display original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 59\u001b[0;31m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[1;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[1;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 20, not 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoder.predict(x_train)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 8))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You will train an autoencoder at some point in the near future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval with Autoencoders (Learn)\n",
    "<a id=\"p3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "A common usecase for autoencoders is for reverse image search. Let's try to draw an image and see what's most similiar in our dataset. \n",
    "\n",
    "To accomplish this we will need to slice our autoendoer in half to extract our reduced features. :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow Along"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(input_img, encoded)\n",
    "encoded_imgs = encoder.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.39389127,  1.0158104 ,  0.        ,  0.        ,  0.06517133,\n",
       "        2.450819  ,  0.        ,  5.1117034 ,  0.74338543,  2.3620906 ,\n",
       "        0.        ,  0.        ,  0.        ,  2.0215404 ,  6.1629906 ,\n",
       "        0.6670714 ,  4.66508   ,  2.5439487 , 17.914988  ,  0.        ,\n",
       "        7.9524546 ,  5.3824563 ,  1.0916216 ,  6.234546  ,  0.        ,\n",
       "        0.8884269 ,  7.485719  ,  3.44194   ,  8.927442  ,  0.        ,\n",
       "        0.23894644,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_imgs[0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10, algorithm='ball_tree')\n",
    "nn.fit(encoded_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.kneighbors(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "You should already be familiar with KNN and similarity queries, so the key component of this section is know what to 'slice' from your autoencoder (the encoder) to extract features from your data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "* <a href=\"#p1\">Part 1</a>: Describe the componenets of an autoencoder\n",
    "    - Enocder\n",
    "    - Decoder\n",
    "* <a href=\"#p2\">Part 2</a>: Train an autoencoder\n",
    "    - Can do in Keras Easily\n",
    "    - Can use a variety of architectures\n",
    "    - Architectures must follow hourglass shape\n",
    "* <a href=\"#p3\">Part 3</a>: Apply an autoenocder to a basic information retrieval problem\n",
    "    - Extract just the encoder to use for various tasks\n",
    "    - AE ares good for dimensionality reduction, reverse image search, and may more things. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "__References__\n",
    "- [Building Autoencoders in Keras](https://blog.keras.io/building-autoencoders-in-keras.html)\n",
    "- [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do)\n",
    "\n",
    "__Additional Material__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
